{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4276a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import the first CSV file\n",
    "cleaned_df = pd.read_csv('ufc_cleaned.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53c66f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winner</th>\n",
       "      <th>p1_height</th>\n",
       "      <th>p1_weight</th>\n",
       "      <th>p1_reach</th>\n",
       "      <th>p1_dob</th>\n",
       "      <th>p1_slpm</th>\n",
       "      <th>p1_str_acc</th>\n",
       "      <th>p1_sapm</th>\n",
       "      <th>p1_str_def</th>\n",
       "      <th>p1_td_avg</th>\n",
       "      <th>p1_td_acc</th>\n",
       "      <th>p1_td_def</th>\n",
       "      <th>p1_sub_avg</th>\n",
       "      <th>p2_height</th>\n",
       "      <th>p2_weight</th>\n",
       "      <th>p2_reach</th>\n",
       "      <th>p2_dob</th>\n",
       "      <th>p2_slpm</th>\n",
       "      <th>p2_str_acc</th>\n",
       "      <th>p2_sapm</th>\n",
       "      <th>p2_str_def</th>\n",
       "      <th>p2_td_avg</th>\n",
       "      <th>p2_td_acc</th>\n",
       "      <th>p2_td_def</th>\n",
       "      <th>p2_sub_avg</th>\n",
       "      <th>p1_stance_Open Stance</th>\n",
       "      <th>p1_stance_Orthodox</th>\n",
       "      <th>p1_stance_Sideways</th>\n",
       "      <th>p1_stance_Southpaw</th>\n",
       "      <th>p1_stance_Switch</th>\n",
       "      <th>p2_stance_Open Stance</th>\n",
       "      <th>p2_stance_Orthodox</th>\n",
       "      <th>p2_stance_Sideways</th>\n",
       "      <th>p2_stance_Southpaw</th>\n",
       "      <th>p2_stance_Switch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1981-11-25</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.50</td>\n",
       "      <td>3.77</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.3</td>\n",
       "      <td>73.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1984-09-14</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.36</td>\n",
       "      <td>2.66</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>74.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1984-01-07</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.48</td>\n",
       "      <td>2.57</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.1</td>\n",
       "      <td>73.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1987-04-21</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.48</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>67.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2000-01-27</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.46</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.6</td>\n",
       "      <td>66.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1991-06-18</td>\n",
       "      <td>5.02</td>\n",
       "      <td>0.56</td>\n",
       "      <td>4.69</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>66.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1996-12-12</td>\n",
       "      <td>5.03</td>\n",
       "      <td>0.38</td>\n",
       "      <td>5.76</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1995-06-10</td>\n",
       "      <td>4.89</td>\n",
       "      <td>0.53</td>\n",
       "      <td>3.77</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>73.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1964-02-11</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.46</td>\n",
       "      <td>4.30</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.44</td>\n",
       "      <td>2.5</td>\n",
       "      <td>74.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1958-06-08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   winner  p1_height  p1_weight  p1_reach      p1_dob  p1_slpm  p1_str_acc  \\\n",
       "0       0       72.0      170.0      73.0  1981-11-25     5.67        0.50   \n",
       "1       1       74.0      205.0      76.0  1984-01-07     3.72        0.48   \n",
       "2       1       67.0      125.0      70.0  2000-01-27     2.99        0.60   \n",
       "3       1       66.0      115.0      69.0  1996-12-12     5.03        0.38   \n",
       "4       1       73.0      205.0      72.0  1964-02-11     1.47        0.46   \n",
       "\n",
       "   p1_sapm  p1_str_def  p1_td_avg  p1_td_acc  p1_td_def  p1_sub_avg  \\\n",
       "0     3.77        0.60       0.51       0.44       0.58         0.3   \n",
       "1     2.57        0.53       0.88       0.33       0.65         0.1   \n",
       "2     2.69        0.46       2.73       0.44       0.45         1.6   \n",
       "3     5.76        0.44       0.95       0.44       0.47         0.0   \n",
       "4     4.30        0.39       0.94       0.60       0.44         2.5   \n",
       "\n",
       "   p2_height  p2_weight  p2_reach      p2_dob  p2_slpm  p2_str_acc  p2_sapm  \\\n",
       "0       73.0      185.0      76.0  1984-09-14     3.29        0.36     2.66   \n",
       "1       73.0      185.0      75.0  1987-04-21     3.54        0.48     4.05   \n",
       "2       66.0      125.0      68.0  1991-06-18     5.02        0.56     4.69   \n",
       "3       65.0      115.0      67.0  1995-06-10     4.89        0.53     3.77   \n",
       "4       74.0      250.0       NaN  1958-06-08     0.00        0.00     0.00   \n",
       "\n",
       "   p2_str_def  p2_td_avg  p2_td_acc  p2_td_def  p2_sub_avg  \\\n",
       "0        0.60       0.61       1.00       0.66         0.0   \n",
       "1        0.50       1.71       0.24       0.80         0.1   \n",
       "2        0.53       0.00       0.00       0.60         0.2   \n",
       "3        0.66       0.74       0.37       0.65         0.5   \n",
       "4        0.00       0.00       0.00       0.00         0.0   \n",
       "\n",
       "   p1_stance_Open Stance  p1_stance_Orthodox  p1_stance_Sideways  \\\n",
       "0                  False               False               False   \n",
       "1                  False                True               False   \n",
       "2                  False                True               False   \n",
       "3                  False                True               False   \n",
       "4                  False                True               False   \n",
       "\n",
       "   p1_stance_Southpaw  p1_stance_Switch  p2_stance_Open Stance  \\\n",
       "0                True             False                  False   \n",
       "1               False             False                  False   \n",
       "2               False             False                  False   \n",
       "3               False             False                  False   \n",
       "4               False             False                  False   \n",
       "\n",
       "   p2_stance_Orthodox  p2_stance_Sideways  p2_stance_Southpaw  \\\n",
       "0               False               False               False   \n",
       "1               False               False                True   \n",
       "2                True               False               False   \n",
       "3                True               False               False   \n",
       "4               False               False                True   \n",
       "\n",
       "   p2_stance_Switch  \n",
       "0              True  \n",
       "1             False  \n",
       "2             False  \n",
       "3             False  \n",
       "4             False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare cleaned_df for training to get baseline\n",
    "# we drop the collumns again since were using the cleaned data, not the feature engineered data\n",
    "columns_to_drop = [\n",
    "    'p1_KD', 'p2_KD', 'p1_SIG_STR_PCT', 'p2_SIG_STR_PCT', 'p1_TD_PCT', 'p2_TD_PCT', \n",
    "    'p1_SUB_ATT', 'p2_SUB_ATT', 'p1_REV', 'p2_REV', 'p1_CTRL', 'p2_CTRL', \n",
    "    'p1_R1_KD', 'p2_R1_KD', 'p1_R1_SIG_STR_PCT', 'p2_R1_SIG_STR_PCT', \n",
    "    'p1_R1_TD_PCT', 'p2_R1_TD_PCT', 'p1_R1_SUB_ATT', 'p2_R1_SUB_ATT', \n",
    "    'p1_R1_REV', 'p2_R1_REV', 'p1_R1_CTRL', 'p2_R1_CTRL', \n",
    "    'p1_SIG_STR_PCT_DETAILED', 'p2_SIG_STR_PCT_DETAILED', \n",
    "    'p1_R1_SIG_STR_PCT_DETAILED', 'p2_R1_SIG_STR_PCT_DETAILED',\n",
    "    'p1_SIG_STR_LANDED', 'p1_SIG_STR_ATTEMPTED', 'p2_SIG_STR_LANDED', 'p2_SIG_STR_ATTEMPTED',\n",
    "    'p1_TOTAL_STR_LANDED', 'p1_TOTAL_STR_ATTEMPTED', 'p2_TOTAL_STR_LANDED', 'p2_TOTAL_STR_ATTEMPTED',\n",
    "    'p1_TD_LANDED', 'p1_TD_ATTEMPTED', 'p2_TD_LANDED', 'p2_TD_ATTEMPTED',\n",
    "    'p1_R1_SIG_STR_LANDED', 'p1_R1_SIG_STR_ATTEMPTED', 'p2_R1_SIG_STR_LANDED', 'p2_R1_SIG_STR_ATTEMPTED',\n",
    "    'p1_R1_TOTAL_STR_LANDED', 'p1_R1_TOTAL_STR_ATTEMPTED', 'p2_R1_TOTAL_STR_LANDED', 'p2_R1_TOTAL_STR_ATTEMPTED',\n",
    "    'p1_R1_TD_LANDED', 'p1_R1_TD_ATTEMPTED', 'p2_R1_TD_LANDED', 'p2_R1_TD_ATTEMPTED',\n",
    "    'p1_HEAD_LANDED', 'p1_HEAD_ATTEMPTED', 'p2_HEAD_LANDED', 'p2_HEAD_ATTEMPTED',\n",
    "    'p1_BODY_LANDED', 'p1_BODY_ATTEMPTED', 'p2_BODY_LANDED', 'p2_BODY_ATTEMPTED',\n",
    "    'p1_LEG_LANDED', 'p1_LEG_ATTEMPTED', 'p2_LEG_LANDED', 'p2_LEG_ATTEMPTED',\n",
    "    'p1_DISTANCE_LANDED', 'p1_DISTANCE_ATTEMPTED', 'p2_DISTANCE_LANDED', 'p2_DISTANCE_ATTEMPTED',\n",
    "    'p1_CLINCH_LANDED', 'p1_CLINCH_ATTEMPTED', 'p2_CLINCH_LANDED', 'p2_CLINCH_ATTEMPTED',\n",
    "    'p1_GROUND_LANDED', 'p1_GROUND_ATTEMPTED', 'p2_GROUND_LANDED', 'p2_GROUND_ATTEMPTED',\n",
    "    'p1_R1_HEAD_LANDED', 'p1_R1_HEAD_ATTEMPTED', 'p2_R1_HEAD_LANDED', 'p2_R1_HEAD_ATTEMPTED',\n",
    "    'p1_R1_BODY_LANDED', 'p1_R1_BODY_ATTEMPTED', 'p2_R1_BODY_LANDED', 'p2_R1_BODY_ATTEMPTED',\n",
    "    'p1_R1_LEG_LANDED', 'p1_R1_LEG_ATTEMPTED', 'p2_R1_LEG_LANDED', 'p2_R1_LEG_ATTEMPTED',\n",
    "    'p1_R1_DISTANCE_LANDED', 'p1_R1_DISTANCE_ATTEMPTED', 'p2_R1_DISTANCE_LANDED', 'p2_R1_DISTANCE_ATTEMPTED',\n",
    "    'p1_R1_CLINCH_LANDED', 'p1_R1_CLINCH_ATTEMPTED', 'p2_R1_CLINCH_LANDED', 'p2_R1_CLINCH_ATTEMPTED',\n",
    "    'p1_R1_GROUND_LANDED', 'p1_R1_GROUND_ATTEMPTED', 'p2_R1_GROUND_LANDED', 'p2_R1_GROUND_ATTEMPTED',\n",
    "    'event_date', 'method'\n",
    "]\n",
    "\n",
    "# Drop the columns\n",
    "cleaned_df = cleaned_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Clean collumn names\n",
    "def clean_column_name(col):\n",
    "    return col.lower().replace(' ', '_').replace('.', '').replace('-', '_')\n",
    "\n",
    "# Apply to all columns\n",
    "cleaned_df.columns = [clean_column_name(col) for col in cleaned_df.columns]\n",
    "\n",
    "# Identify all categorical columns\n",
    "categorical_cols = ['p1_stance', 'p2_stance']\n",
    "\n",
    "# One-hot encode all categorical variables\n",
    "cleaned_df = pd.get_dummies(cleaned_df, columns=categorical_cols)\n",
    "\n",
    "#drop collumns\n",
    "cleaned_df = cleaned_df.drop(['p1_fighter', 'p2_fighter', 'referee'], axis=1)\n",
    "\n",
    "cleaned_df = cleaned_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6843fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 4187, number of negative: 2379\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000998 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2385\n",
      "[LightGBM] [Info] Number of data points in the train set: 6566, number of used features: 28\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.637679 -> initscore=0.565304\n",
      "[LightGBM] [Info] Start training from score 0.565304\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "Logistic Regression:\n",
      "Accuracy: 0.6943\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.40      0.50       617\n",
      "         1.0       0.71      0.87      0.78      1025\n",
      "\n",
      "    accuracy                           0.69      1642\n",
      "   macro avg       0.68      0.64      0.64      1642\n",
      "weighted avg       0.69      0.69      0.67      1642\n",
      "\n",
      "\n",
      "Random Forest:\n",
      "Accuracy: 0.6845\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.43      0.51       617\n",
      "         1.0       0.71      0.84      0.77      1025\n",
      "\n",
      "    accuracy                           0.68      1642\n",
      "   macro avg       0.66      0.63      0.64      1642\n",
      "weighted avg       0.67      0.68      0.67      1642\n",
      "\n",
      "\n",
      "XGBoost:\n",
      "Accuracy: 0.6803\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.43      0.50       617\n",
      "         1.0       0.71      0.83      0.76      1025\n",
      "\n",
      "    accuracy                           0.68      1642\n",
      "   macro avg       0.66      0.63      0.63      1642\n",
      "weighted avg       0.67      0.68      0.67      1642\n",
      "\n",
      "\n",
      "LightGBM:\n",
      "Accuracy: 0.6821\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.44      0.51       617\n",
      "         1.0       0.71      0.83      0.76      1025\n",
      "\n",
      "    accuracy                           0.68      1642\n",
      "   macro avg       0.66      0.63      0.64      1642\n",
      "weighted avg       0.67      0.68      0.67      1642\n",
      "\n",
      "\n",
      "CatBoost:\n",
      "Accuracy: 0.6839\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.39      0.48       617\n",
      "         1.0       0.70      0.86      0.77      1025\n",
      "\n",
      "    accuracy                           0.68      1642\n",
      "   macro avg       0.66      0.63      0.63      1642\n",
      "weighted avg       0.67      0.68      0.66      1642\n",
      "\n",
      "\n",
      "Top 10 Important Features:\n",
      "                          RF       XGB  LGBM\n",
      "p2_slpm             0.083537  0.128422   174\n",
      "p2_sapm             0.053162  0.045085   146\n",
      "p1_sapm             0.060578  0.044026   184\n",
      "p2_td_def           0.059600  0.043808   120\n",
      "p2_td_avg           0.053039  0.040742   117\n",
      "p2_td_acc           0.046554  0.038803   110\n",
      "p2_stance_Switch    0.003566  0.037494     7\n",
      "p1_slpm             0.052969  0.037479   170\n",
      "p2_stance_Orthodox  0.006583  0.036982     8\n",
      "p2_str_acc          0.050151  0.036917   110\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Convert date columns to datetime\n",
    "cleaned_df['p1_dob'] = pd.to_datetime(cleaned_df['p1_dob'], errors='coerce')\n",
    "cleaned_df['p2_dob'] = pd.to_datetime(cleaned_df['p2_dob'], errors='coerce')\n",
    "\n",
    "# Drop date columns and method columns (to prevent data leakage)\n",
    "method_cols = [col for col in cleaned_df.columns if col.startswith('method_')]\n",
    "cleaned_df = cleaned_df.drop(columns=['p1_dob', 'p2_dob'] + method_cols)\n",
    "\n",
    "# Separate numeric and boolean columns\n",
    "numeric_cols = cleaned_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "bool_cols = cleaned_df.select_dtypes(include=['bool']).columns.tolist()\n",
    "\n",
    "# Impute numeric columns\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "cleaned_df[numeric_cols] = numeric_imputer.fit_transform(cleaned_df[numeric_cols])\n",
    "\n",
    "# Convert boolean columns to int before imputation\n",
    "if bool_cols:\n",
    "    for col in bool_cols:\n",
    "        cleaned_df[col] = cleaned_df[col].astype(int)\n",
    "    bool_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    cleaned_df[bool_cols] = bool_imputer.fit_transform(cleaned_df[bool_cols])\n",
    "    # Optional: convert back to bool\n",
    "    for col in bool_cols:\n",
    "        cleaned_df[col] = cleaned_df[col].astype(bool)\n",
    "\n",
    "# Prepare features and target\n",
    "X = cleaned_df.drop(columns=['winner'])\n",
    "y = cleaned_df['winner']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "logreg_pred = logreg.predict(X_test_scaled)\n",
    "results['Logistic Regression'] = {\n",
    "    'accuracy': accuracy_score(y_test, logreg_pred),\n",
    "    'report': classification_report(y_test, logreg_pred)\n",
    "}\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "results['Random Forest'] = {\n",
    "    'accuracy': accuracy_score(y_test, rf_pred),\n",
    "    'report': classification_report(y_test, rf_pred)\n",
    "}\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "results['XGBoost'] = {\n",
    "    'accuracy': accuracy_score(y_test, xgb_pred),\n",
    "    'report': classification_report(y_test, xgb_pred)\n",
    "}\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "lgb_model.fit(X_train, y_train)\n",
    "lgb_pred = lgb_model.predict(X_test)\n",
    "results['LightGBM'] = {\n",
    "    'accuracy': accuracy_score(y_test, lgb_pred),\n",
    "    'report': classification_report(y_test, lgb_pred)\n",
    "}\n",
    "\n",
    "# CatBoost\n",
    "from catboost import CatBoostClassifier\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=5,\n",
    "    random_seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "cat_model.fit(X_train, y_train)\n",
    "cat_pred = cat_model.predict(X_test)\n",
    "results['CatBoost'] = {\n",
    "    'accuracy': accuracy_score(y_test, cat_pred),\n",
    "    'report': classification_report(y_test, cat_pred)\n",
    "}\n",
    "\n",
    "# Print results\n",
    "for model_name, model_results in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"Accuracy: {model_results['accuracy']:.4f}\")\n",
    "    print(f\"Classification Report:\\n{model_results['report']}\")\n",
    "\n",
    "# Optional: Get feature importance from tree-based models\n",
    "feature_importance = pd.DataFrame({\n",
    "    'RF': rf.feature_importances_,\n",
    "    'XGB': xgb_model.feature_importances_,\n",
    "    'LGBM': lgb_model.feature_importances_\n",
    "}, index=X.columns).sort_values(by='XGB', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(feature_importance.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
